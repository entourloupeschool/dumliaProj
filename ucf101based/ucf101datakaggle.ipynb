{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-06-03T10:18:57.882427Z","iopub.status.busy":"2023-06-03T10:18:57.881811Z","iopub.status.idle":"2023-06-03T10:19:09.495156Z","shell.execute_reply":"2023-06-03T10:19:09.493861Z","shell.execute_reply.started":"2023-06-03T10:18:57.882392Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["^C\n","Traceback (most recent call last):\n","  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n","    exec(code, run_globals)\n","  File \"/opt/conda/lib/python3.10/site-packages/pip/__main__.py\", line 29, in <module>\n","    from pip._internal.cli.main import main as _main\n","  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/cli/main.py\", line 10, in <module>\n","    from pip._internal.cli.autocompletion import autocomplete\n","  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n","    from pip._internal.cli.main_parser import create_main_parser\n","  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n","    from pip._internal.build_env import get_runnable_pip\n","  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/build_env.py\", line 21, in <module>\n","    from pip._internal.metadata import get_default_environment, get_environment\n","  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/metadata/__init__.py\", line 9, in <module>\n","    from .base import BaseDistribution, BaseEnvironment, FilesystemWheel, MemoryWheel, Wheel\n","  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/metadata/base.py\", line 32, in <module>\n","    from pip._internal.models.direct_url import (\n","  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n","  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n","  File \"<frozen importlib._bootstrap_external>\", line 975, in get_code\n","  File \"<frozen importlib._bootstrap_external>\", line 1074, in get_data\n","KeyboardInterrupt\n","Note: you may need to restart the kernel to use updated packages.\n","Collecting moviepy\n","  Downloading moviepy-1.0.3.tar.gz (388 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.3/388.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting decorator<5.0,>=4.0.2 (from moviepy)\n","  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /opt/conda/lib/python3.10/site-packages (from moviepy) (4.64.1)\n","Requirement already satisfied: requests<3.0,>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from moviepy) (2.28.2)\n","Collecting proglog<=1.0.0 (from moviepy)\n","  Downloading proglog-0.1.10-py3-none-any.whl (6.1 kB)\n","Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from moviepy) (1.23.5)\n","Requirement already satisfied: imageio<3.0,>=2.5 in /opt/conda/lib/python3.10/site-packages (from moviepy) (2.28.1)\n","Collecting imageio_ffmpeg>=0.2.0 (from moviepy)\n","  Downloading imageio_ffmpeg-0.4.8-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio<3.0,>=2.5->moviepy) (9.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2023.5.7)\n","Building wheels for collected packages: moviepy\n","  Building wheel for moviepy (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for moviepy: filename=moviepy-1.0.3-py3-none-any.whl size=110744 sha256=f98862f03fc0a0c1e0b408a9ec8966e2de8b6938bd4efe5ab5d0af0bd420c3a6\n","  Stored in directory: /root/.cache/pip/wheels/96/32/2d/e10123bd88fbfc02fed53cc18c80a171d3c87479ed845fa7c1\n","Successfully built moviepy\n","^C\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install av\n","%pip install moviepy"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T21:31:39.323604Z","iopub.status.busy":"2023-06-02T21:31:39.323062Z","iopub.status.idle":"2023-06-02T21:31:39.340265Z","shell.execute_reply":"2023-06-02T21:31:39.338775Z","shell.execute_reply.started":"2023-06-02T21:31:39.323564Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["This machine has 8 CPU cores.\n","cpu\n"]}],"source":["import os\n","import torch\n","from torch.utils.data import Dataset\n","import numpy as np\n","from PIL import Image\n","import av\n","import random as rdm\n","import torchvision.transforms.v2 as transforms\n","from tqdm import tqdm\n","import datetime\n","import os\n","import torch\n","import torch.nn.functional as F\n","\n","from torch.utils.data import Dataset\n","from torchvision.io import read_video\n","from torch.utils.data import DataLoader\n","import torchvision\n","import multiprocessing\n","import time\n","from torch.nn.utils.rnn import pad_sequence\n","from torchvision.datasets import UCF101\n","\n","num_cores = multiprocessing.cpu_count()\n","\n","print(\"This machine has {} CPU cores.\".format(num_cores))\n","\n","def set_seed(seed=42):\n","    rdm.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    return seed\n","\n","set_seed()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":86,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T21:31:39.342523Z","iopub.status.busy":"2023-06-02T21:31:39.342126Z","iopub.status.idle":"2023-06-02T21:31:41.051276Z","shell.execute_reply":"2023-06-02T21:31:41.049634Z","shell.execute_reply.started":"2023-06-02T21:31:39.342472Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["zsh:1: command not found: wget\n"]}],"source":["! wget https://github.com/rdfia/rdfia.github.io/raw/master/code/2-cd/utils.py\n","from utils import *"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T21:31:41.055463Z","iopub.status.busy":"2023-06-02T21:31:41.055019Z","iopub.status.idle":"2023-06-02T21:31:41.313887Z","shell.execute_reply":"2023-06-02T21:31:41.312900Z","shell.execute_reply.started":"2023-06-02T21:31:41.055420Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["FPS:  25.0\n","Height:  240\n","Width:  320\n","Ratio height/width:  0.75\n"]}],"source":["# histogram with the distribution of the number of frames per video\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from moviepy.editor import VideoFileClip\n","\n","#file = \"/kaggle/input/ucf101/UCF101/UCF-101/ApplyEyeMakeup/v_ApplyEyeMakeup_g01_c02.avi\"\n","file = \"/Users/in_vite/pCloud Drive/pCloud stockageUtile/Documents/DU MLIA/data/UCF101/SoccerPenalty/v_SoccerPenalty_g01_c04.avi\"\n","\n","clip = VideoFileClip(file)\n","\n","print(\"FPS: \", clip.fps)\n","print(\"Height: \", clip.h)\n","print(\"Width: \", clip.w)\n","ratio_height_width = clip.h / clip.w\n","print(\"Ratio height/width: \", ratio_height_width)"]},{"cell_type":"code","execution_count":88,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T21:31:41.315931Z","iopub.status.busy":"2023-06-02T21:31:41.315419Z","iopub.status.idle":"2023-06-02T21:31:41.327920Z","shell.execute_reply":"2023-06-02T21:31:41.326601Z","shell.execute_reply.started":"2023-06-02T21:31:41.315896Z"},"trusted":true},"outputs":[],"source":["class PermuteTensor(object):\n","    def __call__(self, img):\n","        return img.permute(1, 0, 2, 3)\n","\n","def my_collate_fn(batch):\n","    # Check shape of each video tensor in the batch\n","    for item in batch:\n","        video, audio, label = item\n","        print('Video shape:', video.shape)\n","\n","    # Now let's collate the batch as usual\n","    videos = torch.stack([item[0] for item in batch])\n","    labels = [item[1] for item in batch]\n","    return videos, labels\n","    \n","def get_ucf101(data_path, annotation_path, resized, num_frames=22, fold=1):\n","    col_jit = 0.2\n","    transform_outer = transforms.Compose([\n","        transforms.ColorJitter(brightness=col_jit, contrast=col_jit, saturation=col_jit, hue=col_jit),\n","        transforms.RandomHorizontalFlip(),\n","    ])\n","\n","    transform_inner = transforms.Compose([\n","        PermuteTensor(),\n","        transforms.Resize(resized, antialias=True),\n","        #transforms.Lambda(resize_video),  # Resize videos\n","        transforms.ToImageTensor(),\n","        transforms.ConvertImageDtype(torch.float32),\n","        transforms.Normalize([0.5], [0.5]),\n","        #transforms.Lambda(pad_sequence)  # Custom padding\n","    ])\n","\n","    transform_train = transforms.Compose([\n","        transform_inner,\n","        #transform_outer,\n","        #finalPermuteTensor()\n","    ])\n","\n","    transform_test = transforms.Compose([\n","        transform_inner\n","    ])\n","    \n","    train_dataset = UCF101(data_path, annotation_path, num_frames, 1, fold=fold, transform=transform_train, output_format='TCHW')\n","    test_dataset = UCF101(data_path, annotation_path, num_frames, 1, fold=fold, train=False, transform=transform_test, output_format='TCHW')\n","    \n","    \n","    return train_dataset, test_dataset\n","    #return test_dataset"]},{"cell_type":"code","execution_count":89,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T21:31:41.330058Z","iopub.status.busy":"2023-06-02T21:31:41.329676Z","iopub.status.idle":"2023-06-02T21:39:58.294960Z","shell.execute_reply":"2023-06-02T21:39:58.293382Z","shell.execute_reply.started":"2023-06-02T21:31:41.330027Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 43/43 [00:26<00:00,  1.60it/s]\n","100%|██████████| 43/43 [00:26<00:00,  1.61it/s]\n"]}],"source":["# Test the dataloader & save it in a file\n","#data_path = '/kaggle/input/ucf101/UCF101/UCF-101'\n","data_path = '/Users/in_vite/pCloud Drive/pCloud stockageUtile/Documents/DU MLIA/data/limitedUCF101'\n","#annotation_path = '/kaggle/input/ucf101/UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist'\n","annotation_path = '/Users/in_vite/pCloud Drive/pCloud stockageUtile/Documents/DU MLIA/data/ucfTrainTestList'\n","\n","width = 110\n","height = int(width * ratio_height_width)\n","resize = (height, width)\n","frames = 16\n","fold = 1\n","\n","train_set, valid_set = get_ucf101(data_path, annotation_path, resize, frames, fold)\n","#train_set = get_ucf101(data_path, annotation_path, resize, frames, fold)"]},{"cell_type":"code","execution_count":90,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T21:49:03.812100Z","iopub.status.busy":"2023-06-02T21:49:03.811582Z","iopub.status.idle":"2023-06-02T21:49:14.084973Z","shell.execute_reply":"2023-06-02T21:49:14.083578Z","shell.execute_reply.started":"2023-06-02T21:49:03.812065Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["shape of batch : torch.Size([3, 16, 82, 110])\n","0\n","shape of batch : torch.Size([3, 16, 82, 110])\n","0\n","shape of batch : torch.Size([3, 16, 82, 110])\n","0\n","shape of batch : torch.Size([3, 16, 82, 110])\n","0\n","shape of batch : torch.Size([3, 16, 82, 110])\n","0\n","shape of batch : torch.Size([3, 16, 82, 110])\n","0\n","shape of batch : torch.Size([4, 3, 16, 82, 110])\n","tensor([0, 1, 1, 2])\n","shape of batch : torch.Size([4, 3, 16, 82, 110])\n","tensor([1, 1, 3, 1])\n","shape of batch : torch.Size([4, 3, 16, 82, 110])\n","tensor([1, 1, 1, 1])\n","shape of batch : torch.Size([4, 3, 16, 82, 110])\n","tensor([1, 2, 2, 2])\n","shape of batch : torch.Size([4, 3, 16, 82, 110])\n","tensor([3, 4, 1, 1])\n","shape of batch : torch.Size([4, 3, 16, 82, 110])\n","tensor([3, 2, 0, 4])\n"]}],"source":["for i, (videos, audio, labels) in enumerate(train_set):\n","    print('shape of batch :', videos.shape) # should get [batch_size, 3, seq_len, resized[0], resized[1]]\n","    print(labels) # should get [batch_size], the labels of the videos in the batch\n","    if i == 2:\n","        break\n","    \n","for i, (videos, audio, labels) in enumerate(valid_set):\n","    print('shape of batch :', videos.shape) # should get [batch_size, 3, seq_len, resized[0], resized[1]]\n","    print(labels) # should get [batch_size], the labels of the videos in the batch\n","    if i == 2:\n","        break\n","    \n","def my_collate_fn(batch):\n","    videos = torch.stack([item[0] for item in batch])\n","    labels = torch.Tensor([item[2] for item in batch]).long()  # Index at 2 instead of 1 because you also have 'audio' at index 1\n","    return videos, labels\n","\n","\n","train_loader = DataLoader(train_set, batch_size=4, shuffle=True, collate_fn=my_collate_fn)\n","valid_loader = DataLoader(valid_set, batch_size=4, shuffle=True, collate_fn=my_collate_fn)\n","\n","for i, (videos, labels) in enumerate(train_loader):\n","    print('shape of batch :', videos.shape) # should get [batch_size, 3, seq_len, resized[0], resized[1]]\n","    print(labels) # should get [batch_size], the labels of the videos in the batch\n","    if i == 2:\n","        break\n","for i, (videos, labels) in enumerate(valid_loader):\n","    print('shape of batch :', videos.shape) # should get [batch_size, 3, seq_len, resized[0], resized[1]]\n","    print(labels) # should get [batch_size], the labels of the videos in the batch\n","    if i == 2:\n","        break"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T18:50:30.861434Z","iopub.status.busy":"2023-06-02T18:50:30.860880Z","iopub.status.idle":"2023-06-02T18:50:33.006671Z","shell.execute_reply":"2023-06-02T18:50:33.005204Z","shell.execute_reply.started":"2023-06-02T18:50:30.861382Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory ‘prep_data_folder’: File exists\n"]}],"source":["%mkdir prep_data_folder\n","torch.save(train_set, './prep_data_folder/trainset.pt')\n","torch.save(valid_set, './prep_data_folder/validset.pt')"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T20:41:25.121887Z","iopub.status.busy":"2023-06-02T20:41:25.121466Z","iopub.status.idle":"2023-06-02T20:41:28.195780Z","shell.execute_reply":"2023-06-02T20:41:28.194145Z","shell.execute_reply.started":"2023-06-02T20:41:25.121855Z"},"trusted":true},"outputs":[{"ename":"RuntimeError","evalue":"stack expects each tensor to be equal size, but got [2, 57600] at entry 0 and [1, 0] at entry 1","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[47], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m valid_loader \u001b[38;5;241m=\u001b[39m DataLoader(valid_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (videos, audio, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape of batch :\u001b[39m\u001b[38;5;124m'\u001b[39m, videos\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;66;03m# should get [batch_size, 3, seq_len, resized[0], resized[1]]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(labels) \u001b[38;5;66;03m# should get [batch_size], the labels of the videos in the batch\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:264\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [2, 57600] at entry 0 and [1, 0] at entry 1"]}],"source":["train_set = torch.load('./prep_data_folder/trainset.pt')\n","valid_set = torch.load('./prep_data_folder/validset.pt')\n","train_loader = DataLoader(train_set, batch_size=4, shuffle=True)\n","valid_loader = DataLoader(valid_set, batch_size=4, shuffle=False)\n","for i, (videos, audio, labels) in enumerate(train_loader):\n","    print('shape of batch :', videos.shape) # should get [batch_size, 3, seq_len, resized[0], resized[1]]\n","    print(labels) # should get [batch_size], the labels of the videos in the batch\n","    if i == 2:\n","        break\n","        \n","for i, (videos, audio, labels) in enumerate(valid_loader):\n","    print('shape of batch :', videos.shape) # should get [batch_size, 3, seq_len, resized[0], resized[1]]\n","    print(labels.shape) # should get [batch_size], the labels of the videos in the batch\n","    if i == 2:\n","        break\n","        \n","del train_set, valid_set"]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T07:54:09.512033Z","iopub.status.busy":"2023-06-02T07:54:09.511593Z","iopub.status.idle":"2023-06-02T07:54:09.528247Z","shell.execute_reply":"2023-06-02T07:54:09.527300Z","shell.execute_reply.started":"2023-06-02T07:54:09.512000Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","    \n","class Simple3DCNN(nn.Module):\n","    def __init__(self, num_classes=6, in_channels=3, dropout_prob=0.5):\n","        super(Simple3DCNN, self).__init__()\n","        self.conv1 = nn.Conv3d(in_channels, 32, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n","        self.bn1 = nn.BatchNorm3d(32)\n","        self.relu = nn.ReLU(inplace=True) # ReLU activation\n","        self.maxpool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n","        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n","        self.bn2 = nn.BatchNorm3d(64)\n","        self.maxpool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n","        self.conv3 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n","        self.bn3 = nn.BatchNorm3d(128)\n","        self.maxpool3 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n","        self.dropout = nn.Dropout(dropout_prob)\n","        self.fc1 = nn.Linear(3200, 2816//2)\n","        self.fc2 = nn.Linear(2816//2, num_classes)\n","        \n","    def forward(self, x):\n","        # print(x.shape)\n","        x = self.conv1(x)\n","        # print(x.shape)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool1(x)\n","        # print(x.shape)\n","        x = self.conv2(x)\n","        # print(x.shape)\n","        x = self.bn2(x)\n","        x = self.relu(x)\n","        x = self.maxpool2(x)\n","        # print(x.shape)\n","        x = self.conv3(x)\n","        # print(x.shape)\n","        x = self.bn3(x)\n","        x = self.relu(x)\n","        x = self.maxpool3(x)\n","        # print(x.shape)\n","        x = self.dropout(x)\n","        x = x.view(x.shape[0], -1)\n","        # print(x.shape)\n","        x = self.fc2(self.fc1(x))\n","        # print(x.shape)\n","        return x\n","    \n","    def save(self, file_name='model.pth'):\n","        model_folder_path = './model'\n","        if not os.path.exists(model_folder_path):\n","            os.makedirs(model_folder_path)\n","        file_name = os.path.join(model_folder_path, file_name)\n","        torch.save(self.state_dict(), file_name)\n","        "]},{"cell_type":"code","execution_count":83,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T07:54:10.343405Z","iopub.status.busy":"2023-06-02T07:54:10.343035Z","iopub.status.idle":"2023-06-02T07:54:10.357170Z","shell.execute_reply":"2023-06-02T07:54:10.356116Z","shell.execute_reply.started":"2023-06-02T07:54:10.343376Z"},"trusted":true},"outputs":[],"source":["class Metric: \n","    def __init__(self):\n","        self.loss_train = []\n","        self.loss_test = []\n","        self.acc_train = []\n","        self.acc_test = []\n","    \n","def epoch(data, model, criterion, optimizer=None, cuda=False):\n","    \"\"\"\n","    Make a pass (called epoch in English) on the data `data` with the\n","     model `model`. Evaluates `criterion` as loss.\n","     If `optimizer` is given, perform a training epoch using\n","     the given optimizer, otherwise, perform an evaluation epoch (no backward)\n","     of the model.\n","    \"\"\"\n","\n","    # indicates whether the model is in eval or train mode (some layers behave differently in train and eval)\n","    model.eval() if optimizer is None else model.train()\n","\n","    # objects to store metric averages\n","    avg_loss = AverageMeter()\n","    avg_top1_acc = AverageMeter()\n","    avg_top5_acc = AverageMeter()\n","    avg_batch_time = AverageMeter()\n","    global loss_plot\n","\n","    # we iterate on the batches\n","    tic = time.time()\n","    for i, (input, target) in enumerate(tqdm(data)):\n","        \n","        if cuda: # only with GPU, and not with CPU\n","            input = input.cuda()\n","            target = target.cuda()\n","\n","        # forward\n","        output = model(input)\n","        loss = criterion(output, target)\n","\n","        # backward if we are training\n","        if optimizer:\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        # compute metrics\n","        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n","        batch_time = time.time() - tic\n","        tic = time.time()\n","\n","        # update\n","        avg_loss.update(loss.item())\n","        avg_top1_acc.update(prec1.item())\n","        avg_top5_acc.update(prec5.item())\n","        avg_batch_time.update(batch_time)\n","        if optimizer:\n","            loss_plot.update(avg_loss.val)\n","        # print info\n","        #if i % PRINT_INTERVAL == 0:\n","        #    print('[{0:s} Batch {1:03d}/{2:03d}]\\t'\n","        #          'Time {batch_time.val:.3f}s ({batch_time.avg:.3f}s)\\t'\n","        #          'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n","        #          'Prec@1 {top1.val:5.1f} ({top1.avg:5.1f})\\t'\n","        #          'Prec@5 {top5.val:5.1f} ({top5.avg:5.1f})'.format(\n","        #           \"EVAL\" if optimizer is None else \"TRAIN\", i, len(data), batch_time=avg_batch_time, loss=avg_loss,\n","        #           top1=avg_top1_acc, top5=avg_top5_acc))\n","            #if optimizer:\n","                #loss_plot.plot()\n","\n","    # Print summary\n","    #print('\\n===============> Total time {batch_time:d}s\\t'\n","    #      'Avg loss {loss.avg:.4f}\\t'\n","    #      'Avg Prec@1 {top1.avg:5.2f} %\\t'\n","    #      'Avg Prec@5 {top5.avg:5.2f} %\\n'.format(\n","    #       batch_time=int(avg_batch_time.sum), loss=avg_loss,\n","    #       top1=avg_top1_acc, top5=avg_top5_acc))\n","\n","    return avg_top1_acc, avg_top5_acc, avg_loss"]},{"cell_type":"code","execution_count":84,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T07:54:12.620939Z","iopub.status.busy":"2023-06-02T07:54:12.619777Z","iopub.status.idle":"2023-06-02T07:54:12.632757Z","shell.execute_reply":"2023-06-02T07:54:12.631793Z","shell.execute_reply.started":"2023-06-02T07:54:12.620898Z"},"trusted":true},"outputs":[],"source":["from torch.optim import lr_scheduler\n","import torch.backends.cudnn as cudnn\n","\n","def main(train, test, lr=0.1, epochs=5, cuda=False, dropout_prob=0.1, output_dir='./'):\n","\n","    # define model, loss, optim\n","    model = Simple3DCNN(dropout_prob=dropout_prob)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.SGD(model.parameters(), lr)\n","\n","    scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n","\n","    if cuda: # only with GPU, and not with CPU\n","        cudnn.benchmark = True\n","        model = model.cuda()\n","        criterion = criterion.cuda()\n","\n","    # init plots\n","    listm = []\n","    plot = AccLossPlot()\n","    global loss_plot\n","    loss_plot = TrainLossPlot()\n","    accs_train= []\n","    accs_test= []\n","\n","    # We iterate on the epochs\n","    for i in range(epochs):\n","        m = Metric()\n","\n","        # Train phase\n","        top1_acc, avg_top5_acc, loss = epoch(train, model, criterion, optimizer, cuda)\n","        # Update learning rate\n","        scheduler.step()\n","\n","        # Test phase\n","        top1_acc_test, top5_acc_test, loss_test = epoch(test, model, criterion, cuda=cuda)\n","        # plot\n","        plot.update(loss.avg, loss_test.avg, top1_acc.avg, top1_acc_test.avg)\n","        m.acc_train = top1_acc.avg\n","        m.acc_test = top1_acc_test.avg\n","        m.loss_train = loss.avg\n","        m.loss_test = loss_test.avg\n","        listm.append(m)\n","        print( f\"********** EPOCH {i+1} acc train={m.acc_train:.2f}%, acc test={m.acc_test:.2f}%, loss train={m.loss_train:.3f}, loss test={m.loss_test:.3f} **********\")\n","        accs_train.append(top1_acc)\n","        accs_test.append(top1_acc_test)\n","    \n","    # save model\n","    time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n","    model_file_name = \"model_\" + time + \".pth\"\n","    model.save(output_dir+model_file_name)\n","    \n","    return listm"]},{"cell_type":"code","execution_count":91,"metadata":{"execution":{"iopub.execute_input":"2023-06-02T07:55:56.295640Z","iopub.status.busy":"2023-06-02T07:55:56.295268Z","iopub.status.idle":"2023-06-02T07:56:02.314361Z","shell.execute_reply":"2023-06-02T07:56:02.312606Z","shell.execute_reply.started":"2023-06-02T07:55:56.295611Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/15013 [00:01<?, ?it/s]\n"]},{"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (4x66560 and 3200x1408)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[91], line 22\u001b[0m\n\u001b[1;32m     16\u001b[0m num_frames \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[39m# Get the data\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m#train, test, _ = get_loaders(data_path, sequences_path, resized, batch_size, num_frames)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m#train = torch.load('/kaggle/working/prep_data_folder/trainset.pt')\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m#test = torch.load('/kaggle/working/prep_data_folder/validset.pt')\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m main(train_loader, valid_loader, lr, epochs, cuda, dropout_prob, output_dir)\n","Cell \u001b[0;32mIn[84], line 31\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(train, test, lr, epochs, cuda, dropout_prob, output_dir)\u001b[0m\n\u001b[1;32m     28\u001b[0m m \u001b[39m=\u001b[39m Metric()\n\u001b[1;32m     30\u001b[0m \u001b[39m# Train phase\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m top1_acc, avg_top5_acc, loss \u001b[39m=\u001b[39m epoch(train, model, criterion, optimizer, cuda)\n\u001b[1;32m     32\u001b[0m \u001b[39m# Update learning rate\u001b[39;00m\n\u001b[1;32m     33\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n","Cell \u001b[0;32mIn[83], line 36\u001b[0m, in \u001b[0;36mepoch\u001b[0;34m(data, model, criterion, optimizer, cuda)\u001b[0m\n\u001b[1;32m     33\u001b[0m     target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     35\u001b[0m \u001b[39m# forward\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m output \u001b[39m=\u001b[39m model(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     37\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[1;32m     39\u001b[0m \u001b[39m# backward if we are training\u001b[39;00m\n","File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[82], line 43\u001b[0m, in \u001b[0;36mSimple3DCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x))\n\u001b[1;32m     44\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m x\n","File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x66560 and 3200x1408)"]},{"data":{"text/plain":["<Figure size 640x480 with 0 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<Figure size 640x480 with 0 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["from utils import *\n","#data_path = '/Users/in_vite/pCloud Drive/pCloud stockageUtile/Documents/DU MLIA/data/KTH'\n","#sequences_path = '/Users/in_vite/pCloud Drive/pCloud stockageUtile/Documents/DU MLIA/data/KTH/sequences.txt'\n","output_dir = '/kaggle/working/'\n","width = 12\n","height = int(width * ratio_height_width)\n","resized = (height, width)\n","batch_size = 2\n","\n","lr = 0.0001\n","epochs = 2\n","cuda = torch.cuda.is_available()\n","num_workers = 0\n","dropout_prob = 0.1\n","\n","num_frames = 100\n","\n","# Get the data\n","#train, test, _ = get_loaders(data_path, sequences_path, resized, batch_size, num_frames)\n","#train = torch.load('/kaggle/working/prep_data_folder/trainset.pt')\n","#test = torch.load('/kaggle/working/prep_data_folder/validset.pt')\n","main(train_loader, valid_loader, lr, epochs, cuda, dropout_prob, output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":4}
