{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport torch\nimport torchvision.transforms as transforms\nimport torchvision.transforms.v2 as transformsv2\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.io import read_video\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-05-13T20:19:56.369603Z","iopub.execute_input":"2023-05-13T20:19:56.370274Z","iopub.status.idle":"2023-05-13T20:19:56.376771Z","shell.execute_reply.started":"2023-05-13T20:19:56.370239Z","shell.execute_reply":"2023-05-13T20:19:56.375732Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"class UCF101Dataset(Dataset):\n    def __init__(self, video_list, labels, root_dir, transform=None):\n        self.video_list = video_list\n        self.labels = labels\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.video_list)\n\n    def __getitem__(self, idx):\n        video_name = self.video_list[idx]\n        label = self.labels[video_name]\n        video_path = os.path.join(self.root_dir, video_name)\n        video_frames, _, _ = read_video(video_path, pts_unit=\"sec\")\n\n        if self.transform:\n            video_frames = torch.stack([self.transform(frame) for frame in video_frames])\n\n        return video_frames, label","metadata":{"execution":{"iopub.status.busy":"2023-05-13T20:19:57.586858Z","iopub.execute_input":"2023-05-13T20:19:57.587212Z","iopub.status.idle":"2023-05-13T20:19:57.595145Z","shell.execute_reply.started":"2023-05-13T20:19:57.587185Z","shell.execute_reply":"2023-05-13T20:19:57.594134Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# Load the dataset\nroot_dir = \"/kaggle/input/ucf101/UCF101/UCF-101\"\n\n# Assign labels to the videos\nlabels = {}\nvideos = []\n\nfor action_folder in os.listdir(root_dir):\n    action_folder_path = os.path.join(root_dir, action_folder)\n    \n    if os.path.isdir(action_folder_path):\n        for video in os.listdir(action_folder_path):\n            video_path = os.path.join(action_folder_path, video)\n            \n            if os.path.isfile(video_path):\n                labels[video_path] = action_folder\n                videos.append(video_path)\n\n# Create train-test splits\ntrain_videos, test_videos = train_test_split(videos, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-05-13T20:19:58.538305Z","iopub.execute_input":"2023-05-13T20:19:58.538677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Resize\nresize = 112\n\n# Color Jetter transformation\ncoljit = 0.1\n\n# normalization parameters\nmean = (0.5, 0.5, 0.5)\nstd = (0.5, 0.5, 0.5)\n\ninner_transforms = transformsv2.Compose([\n    transformsv2.Resize(resize),\n    transformsv2.Normalize(mean, std),\n    transformsv2.ToImageTensor(),\n    transformsv2.ConvertImageDtype(torch.float32)\n])\n\nouter_transforms = transformsv2.Compose([\n    transformsv2.RandomHorizontalFlip(),\n    transformsv2.ColorJitter(brightness=coljit, contrast=coljit, saturation=coljit, hue=coljit),\n])\n\n# define the v2 transformations to be applied to the images\ntransform_val = transforms.Compose([\n    inner_transforms\n])\n\ntransform_train = transforms.Compose([\n    outer_transforms,\n    inner_transforms\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_actions = ['ApplyEyeMakeup', 'BenchPress', 'CliffDiving']  # Replace these with the actual action names you want to keep\n\nfiltered_train_videos = [video_path for video_path in train_videos if labels[video_path] in selected_actions]\nfiltered_train_labels = [label for label in labels if label in selected_actions]\n\nfiltered_test_videos = [video_path for video_path in test_videos if labels[video_path] in selected_actions]\nfiltered_test_labels = [label for label in labels if label in selected_actions]\n\n# Create the train and test datasets\ntrain_dataset = UCF101Dataset(filtered_train_videos, filtered_train_labels, root_dir, transform_train)\ntest_dataset = UCF101Dataset(filtered_test_videos, filtered_test_labels, root_dir, transform_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 2\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2023-05-13T20:45:50.829705Z","iopub.execute_input":"2023-05-13T20:45:50.830062Z","iopub.status.idle":"2023-05-13T20:45:50.836261Z","shell.execute_reply.started":"2023-05-13T20:45:50.830033Z","shell.execute_reply":"2023-05-13T20:45:50.835125Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"import torchvision.models as models\n\n# Load the pre-trained 3D ResNet-18 model\nr3d_18 = models.video.r3d_18(weights='DEFAULT')\n\n# Remove the last fully connected layer to use the model for feature extraction\nr3d_18 = torch.nn.Sequential(*list(r3d_18.children())[:-1])\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nr3d_18 = r3d_18.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-13T20:45:51.935710Z","iopub.execute_input":"2023-05-13T20:45:51.936063Z","iopub.status.idle":"2023-05-13T20:45:52.670023Z","shell.execute_reply.started":"2023-05-13T20:45:51.936034Z","shell.execute_reply":"2023-05-13T20:45:52.669058Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"# extract_video_features function\ndef extract_video_features(model, loader):\n    model.eval()\n    features = []\n    labels = []\n\n    with torch.no_grad():\n        for i, (inputs, target) in enumerate(loader):\n            print(f'Extracting video features: {i * batch_size}/{len(loader.dataset)}', end='\\r')\n            inputs = inputs.to(device)\n            labels.append(target)\n\n            # extract features\n            features.append(model(inputs).squeeze())\n\n    return torch.cat(features), torch.cat(labels)","metadata":{"execution":{"iopub.status.busy":"2023-05-13T20:45:53.654699Z","iopub.execute_input":"2023-05-13T20:45:53.655055Z","iopub.status.idle":"2023-05-13T20:45:53.662405Z","shell.execute_reply.started":"2023-05-13T20:45:53.655026Z","shell.execute_reply":"2023-05-13T20:45:53.661508Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# extract features from the train and test datasets\ntrain_features, train_labels = extract_video_features(r3d_18, train_loader)","metadata":{"execution":{"iopub.status.busy":"2023-05-13T20:45:54.893671Z","iopub.execute_input":"2023-05-13T20:45:54.894131Z","iopub.status.idle":"2023-05-13T20:45:55.255891Z","shell.execute_reply.started":"2023-05-13T20:45:54.894093Z","shell.execute_reply":"2023-05-13T20:45:55.252560Z"},"trusted":true},"execution_count":60,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[60], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# extract features from the train and test datasets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_features, train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mextract_video_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr3d_18\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[59], line 8\u001b[0m, in \u001b[0;36mextract_video_features\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m      5\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtracting video features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(loader\u001b[38;5;241m.\u001b[39mdataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n","\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_32/1326434759.py\", line 13, in __getitem__\n    label = self.labels[video_name]\nTypeError: list indices must be integers or slices, not str\n"],"ename":"TypeError","evalue":"Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_32/1326434759.py\", line 13, in __getitem__\n    label = self.labels[video_name]\nTypeError: list indices must be integers or slices, not str\n","output_type":"error"}]},{"cell_type":"code","source":"print(train_labels)","metadata":{"execution":{"iopub.status.busy":"2023-05-13T19:52:53.731131Z","iopub.status.idle":"2023-05-13T19:52:53.732554Z","shell.execute_reply.started":"2023-05-13T19:52:53.732277Z","shell.execute_reply":"2023-05-13T19:52:53.732307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}