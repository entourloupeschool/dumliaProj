{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action Recognition: Recognize human actions or activities in videos. This task requires understanding both spatial and temporal information in video data.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La reconnaissance d'actions est un problème de vision par ordinateur et d'apprentissage automatique qui vise à identifier et classer les actions ou activités effectuées par des personnes dans des vidéos. Voici un aperçu de la façon d'aborder le problème de la reconnaissance d'actions :\n",
    "\n",
    "    Comprendre le problème : La reconnaissance d'actions nécessite de comprendre et d'analyser des informations spatiales (apparence des objets et des personnes) et temporelles (évolution des objets et des personnes dans le temps) contenues dans les vidéos.\n",
    "\n",
    "    Collecte des données : Rassemblez des vidéos contenant des personnes effectuant différentes actions. Vous pouvez utiliser des ensembles de données existants tels que UCF101, HMDB51 et Kinetics. Ces ensembles de données contiennent généralement des annotations fournissant les classes d'actions pour chaque vidéo.\n",
    "\n",
    "    Prétraitement des données : Préparez les données en les normalisant, en les redimensionnant et en les échantillonnant temporellement si nécessaire. L'augmentation des données peut inclure des techniques telles que les retournements, les changements d'éclairage et les modifications de la vitesse de lecture pour améliorer la généralisation du modèle.\n",
    "\n",
    "    Choix du modèle : Sélectionnez un modèle approprié pour la reconnaissance d'actions. Les modèles populaires pour la reconnaissance d'actions incluent les réseaux de neurones convolutifs 3D (3D CNN), les réseaux neuronaux bidirectionnels (Bi-LSTM), les réseaux de neurones convolutifs à déplacement temporel (TSN) et les réseaux Inflated 3D ConvNets (I3D).\n",
    "\n",
    "    Entraînement du modèle : Entraînez le modèle sélectionné sur l'ensemble de données d'entraînement. Pour les modèles d'apprentissage en profondeur, vous devrez peut-être expérimenter différents hyperparamètres tels que le taux d'apprentissage, la taille du lot et le nombre d'époques d'entraînement.\n",
    "\n",
    "    Évaluation du modèle : Évaluez les performances de votre modèle sur un ensemble de données de test distinct. Utilisez des mesures pertinentes telles que la précision, le rappel, le score F1 et la matrice de confusion pour mesurer la qualité de votre modèle.\n",
    "\n",
    "    Amélioration et itération : En fonction des performances de votre modèle, vous devrez peut-être ajuster l'architecture du modèle, les techniques de prétraitement ou les paramètres d'entraînement. N'hésitez pas à expérimenter et à apprendre des erreurs pour améliorer continuellement votre modèle."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Comprendre le problème : La reconnaissance d'actions nécessite de comprendre et d'analyser des informations spatiales (apparence des objets et des personnes) et temporelles (évolution des objets et des personnes dans le temps) contenues dans les vidéos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "UCF101 is an action recognition data set of realistic action videos, collected from YouTube, having 101 action categories. This data set is an extension of UCF50 data set which has 50 action categories.\n",
    "\n",
    "With 13320 videos from 101 action categories, UCF101 gives the largest diversity in terms of actions and with the presence of large variations in camera motion, object appearance and pose, object scale, viewpoint, cluttered background, illumination conditions, etc, it is the most challenging data set to date. As most of the available action recognition data sets are not realistic and are staged by actors, UCF101 aims to encourage further research into action recognition by learning and exploring new realistic action categories.\n",
    "https://www.crcv.ucf.edu/research/data-sets/ucf101/<br> <br>\n",
    "**Data Set Details**\n",
    "\n",
    "The videos in 101 action categories are grouped into 25 groups, where each group can consist of 4-7 videos of an action. The videos from the same group may share some common features, such as similar background, similar viewpoint, etc.\n",
    "\n",
    "The action categories can be divided into five types:\n",
    "\n",
    "    Human-Object Interaction\n",
    "    Body-Motion Only\n",
    "    Human-Human Interaction\n",
    "    Playing Musical Instruments\n",
    "    Sports\n",
    "\n",
    "The action categories for UCF101 data set are: Apply Eye Makeup, Apply Lipstick, Archery, Baby Crawling, Balance Beam, Band Marching, Baseball Pitch, Basketball Shooting, Basketball Dunk, Bench Press, Biking, Billiards Shot, Blow Dry Hair, Blowing Candles, Body Weight Squats, Bowling, Boxing Punching Bag, Boxing Speed Bag, Breaststroke, Brushing Teeth, Clean and Jerk, Cliff Diving, Cricket Bowling, Cricket Shot, Cutting In Kitchen, Diving, Drumming, Fencing, Field Hockey Penalty, Floor Gymnastics, Frisbee Catch, Front Crawl, Golf Swing, Haircut, Hammer Throw, Hammering, Handstand Pushups, Handstand Walking, Head Massage, High Jump, Horse Race, Horse Riding, Hula Hoop, Ice Dancing, Javelin Throw, Juggling Balls, Jump Rope, Jumping Jack, Kayaking, Knitting, Long Jump, Lunges, Military Parade, Mixing Batter, Mopping Floor, Nun chucks, Parallel Bars, Pizza Tossing, Playing Guitar, Playing Piano, Playing Tabla, Playing Violin, Playing Cello, Playing Daf, Playing Dhol, Playing Flute, Playing Sitar, Pole Vault, Pommel Horse, Pull Ups, Punch, Push Ups, Rafting, Rock Climbing Indoor, Rope Climbing, Rowing, Salsa Spins, Shaving Beard, Shotput, Skate Boarding, Skiing, Skijet, Sky Diving, Soccer Juggling, Soccer Penalty, Still Rings, Sumo Wrestling, Surfing, Swing, Table Tennis Shot, Tai Chi, Tennis Swing, Throw Discus, Trampoline Jumping, Typing, Uneven Bars, Volleyball Spiking, Walking with a dog, Wall Pushups, Writing On Board, Yo Yo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Collecte des données : utilisation de HCF101 qui est annoté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if HCF101 dataset is downloaded in data, if not download it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # operating system\n",
    "import glob # unix style pathname pattern expansion\n",
    "from torchvision.datasets import VisionDataset # base class for vision datasets\n",
    "from torchvision.datasets.folder import make_dataset # helper function to create a dataset\n",
    "from torchvision.datasets.video_utils import VideoClips # helper class to handle video clips\n",
    "from torchvision.transforms import ToTensor # convert a PIL Image or numpy.ndarray to tensor\n",
    "import torch\n",
    "\n",
    "class UCF101(VisionDataset):\n",
    "    def __init__(self, root, frames_per_clip=16, step_between_clips=1, transform=None, target_transform=None):\n",
    "        super(UCF101, self).__init__(root, transform=transform, target_transform=target_transform)\n",
    "\n",
    "        extensions = ('avi',)\n",
    "        classes = sorted(os.listdir(root))\n",
    "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        self.samples = make_dataset(self.root, class_to_idx, extensions, is_valid_file=None)\n",
    "        video_list = [x[0] for x in self.samples]\n",
    "        self.video_clips = VideoClips(video_list, frames_per_clip, step_between_clips)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.video_clips.num_clips()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video, _, _, video_idx = self.video_clips.get_clip(idx)\n",
    "        target = self.samples[video_idx][1]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            video = self.transform(video)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return video, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
