{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/in_vite/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - torchvision\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    torchvision-0.13.1         |cpu_py310hc47236b_0         6.0 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         6.0 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  torchvision        pkgs/main/osx-64::torchvision-0.13.1-cpu_py310hc47236b_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2022.12.~ --> pkgs/main::ca-certificates-2023.01.10-hecd8cb5_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            conda-forge/noarch::certifi-2022.12.7~ --> pkgs/main/osx-64::certifi-2022.12.7-py310hecd8cb5_0 \n",
      "  openssl            conda-forge::openssl-1.1.1t-hfd90126_0 --> pkgs/main::openssl-1.1.1t-hca72f7f_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "                                                                                \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "# install the clip package through with conda\n",
    "!conda install -y torchvision -c pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - tochvision=0.8.2\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://conda.anaconda.org/pytorch/osx-64\n",
      "  - https://conda.anaconda.org/pytorch/noarch\n",
      "  - https://repo.anaconda.com/pkgs/main/osx-64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/osx-64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n",
      "Collecting openai-clip\n",
      "  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m48.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0mm\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ftfy\n",
      "  Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: regex in /Users/in_vite/anaconda3/lib/python3.10/site-packages (from openai-clip) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/in_vite/anaconda3/lib/python3.10/site-packages (from openai-clip) (4.64.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Users/in_vite/anaconda3/lib/python3.10/site-packages (from ftfy->openai-clip) (0.2.5)\n",
      "Building wheels for collected packages: openai-clip\n",
      "  Building wheel for openai-clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openai-clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368605 sha256=cd6bff139fa2f8833fdb349ba0d301ab3eee282dcaf299485db982ddaa8b249b\n",
      "  Stored in directory: /Users/in_vite/Library/Caches/pip/wheels/25/19/a7/6cd72b592a2529e22a20fd7701330529351171c575745d998e\n",
      "Successfully built openai-clip\n",
      "Installing collected packages: ftfy, openai-clip\n",
      "Successfully installed ftfy-6.1.1 openai-clip-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!conda install -y tochvision=0.8.2 -c pytorch\n",
    "!pip install openai-clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3326551841.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    import openai-clip\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_video\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'clip' has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Load the CLIP model and the associated text tokenizer:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model, transform \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39m\u001b[39mViT-B/32\u001b[39m\u001b[39m\"\u001b[39m, device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'clip' has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "# Load the CLIP model and the associated text tokenizer:\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, transform = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
