{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T20:23:51.204809Z","iopub.status.busy":"2023-04-06T20:23:51.204110Z","iopub.status.idle":"2023-04-06T20:23:51.211919Z","shell.execute_reply":"2023-04-06T20:23:51.209733Z","shell.execute_reply.started":"2023-04-06T20:23:51.204769Z"},"trusted":true},"outputs":[],"source":["import torch\n","\n","def get_dataset(path_train, path_val, batch_size, cuda=False):\n","    \"\"\"\n","    Cette fonction charge le dataset\n","    \"\"\"\n","\n","    train_dataset = torch.load(path_train)\n","    val_dataset = torch.load(path_val)\n","\n","    train_loader = torch.utils.data.DataLoader(train_dataset,\n","                        batch_size=batch_size, shuffle=True, pin_memory=cuda, num_workers=2)\n","    val_loader = torch.utils.data.DataLoader(val_dataset,\n","                        batch_size=batch_size, shuffle=False, pin_memory=cuda, num_workers=2)\n","\n","    return train_loader, val_loader"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T20:23:54.295684Z","iopub.status.busy":"2023-04-06T20:23:54.295213Z","iopub.status.idle":"2023-04-06T20:23:55.713161Z","shell.execute_reply":"2023-04-06T20:23:55.712040Z","shell.execute_reply.started":"2023-04-06T20:23:54.295647Z"},"trusted":true},"outputs":[],"source":["import time\n","\n","from utils import *\n","\n","class Metric: \n","    def __init__(self):\n","        self.loss_train = []\n","        self.loss_test = []\n","        self.acc_train = []\n","        self.acc_test = []\n","        \n","def epoch(data, model, criterion, optimizer=None, cuda=False):\n","    \"\"\"\n","    Make a pass (called epoch in English) on the data `data` with the\n","     model `model`. Evaluates `criterion` as loss.\n","     If `optimizer` is given, perform a training epoch using\n","     the given optimizer, otherwise, perform an evaluation epoch (no backward)\n","     of the model.\n","    \"\"\"\n","\n","    # indicates whether the model is in eval or train mode (some layers behave differently in train and eval)\n","    model.eval() if optimizer is None else model.train()\n","\n","    # objects to store metric averages\n","    avg_loss = AverageMeter()\n","    avg_top1_acc = AverageMeter()\n","    avg_top5_acc = AverageMeter()\n","    avg_batch_time = AverageMeter()\n","\n","    # we iterate on the batches\n","    tic = time.time()\n","    for i, (input, target) in enumerate(data):\n","\n","        if cuda: # only with GPU, and not with CPU\n","            input = input.cuda()\n","            target = target.cuda()\n","\n","        # forward\n","        output = model(input)\n","        loss = criterion(output, target)\n","\n","        # backward if we are training\n","        if optimizer:\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        # compute metrics\n","        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n","        batch_time = time.time() - tic\n","        tic = time.time()\n","\n","        # update\n","        avg_loss.update(loss.item())\n","        avg_top1_acc.update(prec1.item())\n","        avg_top5_acc.update(prec5.item())\n","        avg_batch_time.update(batch_time)\n","        if optimizer:\n","            loss_plot.update(avg_loss.val)\n","\n","    return avg_top1_acc, avg_top5_acc, avg_loss"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T20:23:55.716425Z","iopub.status.busy":"2023-04-06T20:23:55.716012Z","iopub.status.idle":"2023-04-06T20:23:55.732915Z","shell.execute_reply":"2023-04-06T20:23:55.731648Z","shell.execute_reply.started":"2023-04-06T20:23:55.716370Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","\n","def main(train_path, val_path, batch_size=128, lr=0.2, epochs=35, device='cpu', patience=5):\n","    best_dropout_rate = None\n","    best_validation_accuracy = 0.0\n","    best_metrics = None\n","\n","    criterion = nn.CrossEntropyLoss()\n","    \n","    global loss_plot\n","    loss_plot = TrainLossPlot()\n","\n","    # define model, loss, optim\n","    model = CustomNet()\n","    optimizer = torch.optim.Adam(model.parameters(), lr)\n","\n","    # Set the learning rate scheduler\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs//4, eta_min=1/(10**patience)) \n","\n","    if device != 'cpu':\n","        if device == 'cuda':\n","            # Set the cudnn.benchmark flag to True for cudnn to use the built-in autotuner to find the best algorithm for the hardware\n","            torch.backends.cudnn.benchmark = True\n","\n","        # Set the pin_memory flag to True for DataLoader to use pinned (page-locked) memory, which speeds up data transfer between CPU and GPU\n","        pin_memory = True\n","\n","        model = model.to(device)\n","        criterion = criterion.to(device)\n","\n","    # Get the data\n","    train, test = get_dataset(train_path, val_path, batch_size, device)\n","\n","    # Initialize a list to store metrics\n","    listm = []\n","\n","    best_val_loss = float('inf')\n","    epochs_without_improvement = 0\n","\n","    for i in range(epochs):\n","        m = Metric()\n","\n","        # Train phase\n","        top1_acc, avg_top5_acc, loss = epoch(train, model, criterion, optimizer, device)\n","        # Update learning rate\n","        scheduler.step()\n","\n","        # Test phase\n","        top1_acc_test, top5_acc_test, loss_test = epoch(test, model, criterion, device=device)\n","\n","        m.acc_train = top1_acc.avg\n","        m.acc_test = top1_acc_test.avg\n","        m.loss_train = loss.avg\n","        m.loss_test = loss_test.avg\n","        listm.append(m)\n","        print(f\"Epoch {i+1} - acc train={m.acc_train:.2f}%, acc test={m.acc_test:.2f}%, loss train={m.loss_train:.3f}, loss test={m.loss_test:.3f}\")\n","\n","        # Early stopping\n","        if m.loss_test < best_val_loss:\n","            best_val_loss = m.loss_test\n","            epochs_without_improvement = 0\n","            # Save the best model\n","            torch.save(model.state_dict(), 'best_model.pth')\n","        else:\n","            epochs_without_improvement += 1\n","            if epochs_without_improvement >= patience:\n","                print(\"Early stopping triggered\")\n","                break\n","\n","    loss_train= [listm[i].loss_train for i in range(len(listm))]\n","    loss_test= [listm[i].loss_test for i in range(len(listm))]\n","    acc_test= [listm[i].acc_test for i in range(len(listm))]\n","    acc_train= [listm[i].acc_train for i in range(len(listm))]\n","\n","    x = range(1,len(listm)+1)\n","    plt.figure(figsize=(5, 5))\n","    plt.plot(x,loss_train,label='loss train')\n","    plt.plot(x,loss_test,label='loss test')\n","    plt.legend(loc=\"upper right\")\n","    plt.show()\n","    plt.figure(figsize=(10, 10))\n","    plt.plot(x,acc_train,label='acc train')\n","    plt.plot(x,acc_test,label='acc test')\n","    plt.legend(loc=\"upper left\")\n","    plt.show()\n","\n","    return"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T20:24:17.295557Z","iopub.status.busy":"2023-04-06T20:24:17.295065Z","iopub.status.idle":"2023-04-06T20:24:17.328361Z","shell.execute_reply":"2023-04-06T20:24:17.327385Z","shell.execute_reply.started":"2023-04-06T20:24:17.295511Z"},"trusted":true},"outputs":[],"source":["# Define the model\n","# A simple 3D convolutional network with 6 convolutional layers and 2 fully connected layers\n","# The network is defined in the __init__ method\n","# The forward method defines the forward pass of the network\n","# The _make_layer method is used to create the convolutional layers\n","# The _initialize_weights method is used to initialize the weights of the network\n","\n","class CustomNet(nn.Module): # inherit from nn.Module\n","    def __init__(self, num_classes=101): # constructor\n","        super(CustomNet, self).__init__()   # call the constructor of the parent class\n","        self.conv1 = nn.Conv3d(3, 64, kernel_size=3, stride=1, padding=1) # 3 input channels, 64 output channels, 3x3 kernel, stride 1, padding 1\n","        self.bn1 = nn.BatchNorm3d(64) # batch normalization\n","        self.relu = nn.ReLU(inplace=True) # ReLU activation\n","        self.maxpool = nn.MaxPool3d(kernel_size=2, stride=2) # max pooling\n","        self.layer1 = self._make_layer(64, 64, 2) # 2 layers of 64 filters\n","        self.layer2 = self._make_layer(64, 128, 2, stride=2)    # 2 layers of 128 filters, stride 2\n","        self.layer3 = self._make_layer(128, 256, 2, stride=2)  # 2 layers of 256 filters, stride 2\n","        self.layer4 = self._make_layer(256, 512, 2, stride=2) # 2 layers of 512 filters, stride 2\n","        self.avgpool = nn.AvgPool3d((4, 7, 7), stride=1) # average pooling\n","        self.fc = nn.Linear(512 * 1 * 1 * 1, num_classes) # fully connected layer\n","        self._initialize_weights() # initialize the weights\n","\n","    def _make_layer(self, inplanes, planes, blocks, stride=1): # create the convolutional layers\n","        downsample = None # downsample is used to downsample the input to match the output of the convolutional layer\n","        if stride != 1 or inplanes != planes: # if the stride is not 1 or the number of input channels is not equal to the number of output channels\n","            downsample = nn.Sequential( # downsample the input\n","                nn.Conv3d(inplanes, planes, kernel_size=1, stride=stride, bias=False), # 1x1 convolution\n","                nn.BatchNorm3d(planes), # batch normalization\n","            ) \n","\n","        layers = [] # create a list of layers\n","        layers.append(BasicBlock(inplanes, planes, stride, downsample)) # append the first layer\n","        for i in range(1, blocks): # append the remaining layers\n","            layers.append(BasicBlock(planes, planes)) # append the layer\n","\n","        return nn.Sequential(*layers) # return the layers\n","    \n","    def _initialize_weights(self): # initialize the weights\n","        for m in self.modules(): # loop over the layers\n","            if isinstance(m, nn.Conv3d): # if the layer is a convolutional layer\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') # initialize the weights\n","                if m.bias is not None: # if the layer has a bias\n","                    nn.init.constant_(m.bias, 0) # initialize the bias to 0\n","            elif isinstance(m, nn.BatchNorm3d): # if the layer is a batch normalization layer\n","                nn.init.constant_(m.weight, 1) # initialize the weights to 1\n","                nn.init.constant_(m.bias, 0) # initialize the bias to 0\n","            elif isinstance(m, nn.Linear): # if the layer is a fully connected layer\n","                nn.init.normal_(m.weight, 0, 0.01) # initialize the weights to 0\n","                nn.init.constant_(m.bias, 0)    # initialize the bias to 0\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","\n","        return x\n","    \n","\n","# Define the basic block\n","# The basic block is used to create the convolutional layers\n","# The forward method defines the forward pass of the block\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1 # expansion is used to downsample the input to match the output of the convolutional layer\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None): # constructor\n","        super(BasicBlock, self).__init__() # call the constructor of the parent class\n","        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False) # 3x3 convolution\n","        self.bn1 = nn.BatchNorm3d(planes) # batch normalization\n","        self.relu = nn.ReLU(inplace=True) # ReLU activation\n","        self.conv2 = nn.Conv3d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False) # 3x3 convolution\n","        self.bn2 = nn.BatchNorm3d(planes) # batch normalization\n","        self.downsample = downsample # downsample the input\n","        self.stride = stride # stride\n","\n","    def forward(self, x): \n","        residual = x # residual is the input\n","\n","        out = self.conv1(x) # apply the first convolution\n","        out = self.bn1(out) # apply batch normalization\n","        out = self.relu(out) # apply ReLU activation\n","\n","        out = self.conv2(out) # apply the second convolution\n","        out = self.bn2(out) # apply batch normalization\n","\n","        if self.downsample is not None: # if the input needs to be downsampled\n","            residual = self.downsample(x) # downsample the input\n","\n","        out += residual # add the input to the output\n","        out = self.relu(out) # apply ReLU activation\n","\n","        return out"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T20:24:18.545539Z","iopub.status.busy":"2023-04-06T20:24:18.545161Z","iopub.status.idle":"2023-04-06T20:24:18.571447Z","shell.execute_reply":"2023-04-06T20:24:18.570382Z","shell.execute_reply.started":"2023-04-06T20:24:18.545501Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["./data/preparedMac/dataset_frames16_fold1_resize120_coljit0.1/trainset.pt\n","./data/preparedMac/dataset_frames16_fold1_resize120_coljit0.1/valset.pt\n","./data/preparedMac/dataset_frames16_fold2_resize120_coljit0.1/trainset.pt\n","./data/preparedMac/dataset_frames16_fold2_resize120_coljit0.1/valset.pt\n","./data/preparedMac/dataset_frames16_fold3_resize120_coljit0.1/trainset.pt\n","./data/preparedMac/dataset_frames16_fold3_resize120_coljit0.1/valset.pt\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","\n","for dirname, _, filenames in os.walk('./data/preparedMac'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-04-06T20:24:21.627999Z","iopub.status.busy":"2023-04-06T20:24:21.626691Z","iopub.status.idle":"2023-04-06T20:24:25.873544Z","shell.execute_reply":"2023-04-06T20:24:25.872017Z","shell.execute_reply.started":"2023-04-06T20:24:21.627949Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n","  warnings.warn(_BETA_TRANSFORMS_WARNING)\n","/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n","  warnings.warn(_BETA_TRANSFORMS_WARNING)\n","/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n","  warnings.warn(_BETA_TRANSFORMS_WARNING)\n","/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n","  warnings.warn(_BETA_TRANSFORMS_WARNING)\n","/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n","  warnings.warn(_BETA_TRANSFORMS_WARNING)\n","/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n","  warnings.warn(_BETA_TRANSFORMS_WARNING)\n","/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n","  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n","/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n","  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n","/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"ename":"TypeError","evalue":"Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/datasets/ucf101.py\", line 128, in __getitem__\n    video = self.transform(video)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/transforms/v2/_container.py\", line 51, in forward\n    sample = transform(sample)\n             ^^^^^^^^^^^^^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/transforms/v2/_transform.py\", line 44, in forward\n    flat_outputs = [\n                   ^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/transforms/v2/_transform.py\", line 45, in <listcomp>\n    self._transform(inpt, params) if needs_transform else inpt\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/transforms/v2/_misc.py\", line 175, in _transform\n    return F.normalize(inpt, mean=self.mean, std=self.std, inplace=self.inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/transforms/v2/functional/_misc.py\", line 66, in normalize\n    return inpt.normalize(mean=mean, std=std, inplace=inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/datapoints/_image.py\", line 253, in normalize\n    output = self._F.normalize_image_tensor(self.as_subclass(torch.Tensor), mean=mean, std=std, inplace=inplace)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/transforms/v2/functional/_misc.py\", line 20, in normalize_image_tensor\n    raise TypeError(f\"Input tensor should be a float tensor. Got {image.dtype}.\")\nTypeError: Input tensor should be a float tensor. Got torch.uint8.\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m train_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./data/preparedMac/dataset_frames16_fold1_resize120_coljit0.1/trainset.pt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m val_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./data/preparedMac/dataset_frames16_fold1_resize120_coljit0.1/valset.pt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m metrics \u001b[39m=\u001b[39m main(train_path, val_path, batch_size\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m175\u001b[39;49m, cuda\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n","Cell \u001b[0;32mIn[3], line 42\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(train_path, val_path, batch_size, lr, epochs, cuda, patience)\u001b[0m\n\u001b[1;32m     39\u001b[0m m \u001b[39m=\u001b[39m Metric()\n\u001b[1;32m     41\u001b[0m \u001b[39m# Train phase\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m top1_acc, avg_top5_acc, loss \u001b[39m=\u001b[39m epoch(train, model, criterion, optimizer, cuda)\n\u001b[1;32m     43\u001b[0m \u001b[39m# Update learning rate\u001b[39;00m\n\u001b[1;32m     44\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n","Cell \u001b[0;32mIn[2], line 32\u001b[0m, in \u001b[0;36mepoch\u001b[0;34m(data, model, criterion, optimizer, cuda)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m# we iterate on the batches\u001b[39;00m\n\u001b[1;32m     31\u001b[0m tic \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 32\u001b[0m \u001b[39mfor\u001b[39;00m i, (\u001b[39minput\u001b[39m, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data):\n\u001b[1;32m     34\u001b[0m     \u001b[39mif\u001b[39;00m cuda: \u001b[39m# only with GPU, and not with CPU\u001b[39;00m\n\u001b[1;32m     35\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mcuda()\n","File \u001b[0;32m~/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m~/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n","File \u001b[0;32m~/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1373\u001b[0m \u001b[39mreturn\u001b[39;00m data\n","File \u001b[0;32m~/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n","\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/datasets/ucf101.py\", line 128, in __getitem__\n    video = self.transform(video)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/transforms/v2/_container.py\", line 51, in forward\n    sample = transform(sample)\n             ^^^^^^^^^^^^^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/transforms/v2/_transform.py\", line 44, in forward\n    flat_outputs = [\n                   ^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/transforms/v2/_transform.py\", line 45, in <listcomp>\n    self._transform(inpt, params) if needs_transform else inpt\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/transforms/v2/_misc.py\", line 175, in _transform\n    return F.normalize(inpt, mean=self.mean, std=self.std, inplace=self.inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/transforms/v2/functional/_misc.py\", line 66, in normalize\n    return inpt.normalize(mean=mean, std=std, inplace=inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/datapoints/_image.py\", line 253, in normalize\n    output = self._F.normalize_image_tensor(self.as_subclass(torch.Tensor), mean=mean, std=std, inplace=inplace)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/theodoredwernicki/anaconda3/envs/AI-computervision/lib/python3.11/site-packages/torchvision/transforms/v2/functional/_misc.py\", line 20, in normalize_image_tensor\n    raise TypeError(f\"Input tensor should be a float tensor. Got {image.dtype}.\")\nTypeError: Input tensor should be a float tensor. Got torch.uint8.\n"]},{"data":{"text/plain":["<Figure size 640x480 with 0 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["train_path = \"./data/preparedMac/dataset_frames16_fold1_resize120_coljit0.1/trainset.pt\"\n","val_path = \"./data/preparedMac/dataset_frames16_fold1_resize120_coljit0.1/valset.pt\"\n","\n","metrics = main(train_path, val_path, batch_size=3, epochs=50, device='mps')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":4}
